{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbb0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e56b112a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(),'..','data','korpus')\n",
    "subjects = ['Academic','Culture','European','Law','News',\n",
    "            'Opinion','Parliament','Religion','Sport','Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9504e1",
   "metadata": {},
   "source": [
    "## Removing XML tags from <i>korpus</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d8c9f2",
   "metadata": {},
   "source": [
    "All tags except <b>\\<s\\></b> will be removed from the korpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727c7805",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'<(?![/]?s).*?>')\n",
    "def clean(text):\n",
    "    return regex.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eab5052b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic1.txt\n",
      "Academic2.txt\n",
      "Culture1.txt\n",
      "Culture2.txt\n",
      "European.txt\n",
      "Law.txt\n",
      "News.txt\n",
      "Opinion1.txt\n",
      "Parliament.csv\n",
      "Parliament2.txt\n",
      "Religion.csv\n",
      "Religion2.txt\n",
      "Sport.csv\n",
      "Sport1.txt\n",
      "Test.csv\n",
      "Test.txt\n"
     ]
    }
   ],
   "source": [
    "for sbj in subjects:\n",
    "    path = os.path.join(folder,sbj)\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        filepath = os.path.join(path,filename)\n",
    "\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            print(filename)\n",
    "            text = f.read()\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(clean(text))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146385c",
   "metadata": {},
   "source": [
    "## Turning text file into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e26bed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic1.txt\n",
      "Academic2.txt\n",
      "Culture1.txt\n",
      "Culture2.txt\n",
      "European.txt\n",
      "Law.txt\n",
      "News.txt\n",
      "Opinion1.txt\n",
      "Parliament2.txt\n",
      "Religion2.txt\n",
      "Sport1.txt\n",
      "Test.txt\n"
     ]
    }
   ],
   "source": [
    "for sbj in subjects:\n",
    "    data = []\n",
    "    path = os.path.join(folder,sbj)\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        filepath = os.path.join(path,filename)\n",
    "        \n",
    "        with open(filepath,'r', encoding='utf-8') as f:\n",
    "            print(filename)\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            for l in lines:\n",
    "                d = l.split('\\t')\n",
    "                \n",
    "                #Start and End Token\n",
    "                if len(d) == 1:\n",
    "                    if re.search(r'<s id=\"[0-9]*\">', d[0]): data.append(['<s>','START',None,None])\n",
    "                    elif re.search(r'</s>', d[0]):          data.append(['</s>','END',None,None])\n",
    "                \n",
    "                elif len(d) > 1:\n",
    "                    d[-1] = d[-1][:-1]\n",
    "                    data.append(d)\n",
    "                    \n",
    "    df = pd.DataFrame(data, columns=['Word','POS','Lemma','Root'])\n",
    "    df.to_csv(os.path.join(path,sbj+'.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ad781",
   "metadata": {},
   "source": [
    "## Joining individual csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df8658",
   "metadata": {},
   "source": [
    "We will create 2 versions of korpus.csv. One will simply be all the csv files together while the other will read an equal number of bytes from each file. This is so the frequency counts won't be biased based on the subject of the texts.\n",
    "\n",
    "Note that we do not include the text file in the training korpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb41bcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Academic Finished\n",
      "Culture Finished\n",
      "European Finished\n",
      "Law Finished\n",
      "News Finished\n",
      "Opinion Finished\n",
      "Parliament Finished\n",
      "Religion Finished\n",
      "Sport Finished\n",
      "All Finished\n",
      "\n",
      "Academic Finished\n",
      "Culture Finished\n",
      "European Finished\n",
      "Law Finished\n",
      "News Finished\n",
      "Opinion Finished\n",
      "Parliament Finished\n",
      "Religion Finished\n",
      "Sport Finished\n",
      "All Finished\n"
     ]
    }
   ],
   "source": [
    "subjects = ['Academic','Culture','European','Law','News',\n",
    "            'Opinion','Parliament','Religion','Sport',]\n",
    "\n",
    "with open(os.path.join(folder,'korpus.csv'), 'w', encoding='utf-8') as f1:\n",
    "    for sbj in subjects:\n",
    "        with open(os.path.join(folder,sbj,sbj+'.csv'), 'r', encoding='utf-8') as f2:\n",
    "            f1.write('\\n')\n",
    "            f1.write(f2.read())\n",
    "            print(f'{sbj} Finished')\n",
    "\n",
    "print('All Finished\\n')\n",
    "\n",
    "\n",
    "min_size = min([os.path.getsize(os.path.join(folder,sbj,sbj+'.csv')) for sbj in subjects])\n",
    "\n",
    "#Only read min_length lines from each csv file. \n",
    "\n",
    "with open(os.path.join(folder,'norm_korpus.csv'), 'w', encoding='utf-8') as f1:\n",
    "    for sbj in subjects:\n",
    "        with open(os.path.join(folder,sbj,sbj+'.csv'), 'r', encoding='utf-8') as f2:    \n",
    "            f1.write('\\n')\n",
    "            f1.write(f2.read(min_size))\n",
    "            print(f'{sbj} Finished')\n",
    "\n",
    "print('All Finished')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f7a035",
   "metadata": {},
   "source": [
    "## Exploring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a8ad084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 27.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>START</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L-</td>\n",
       "      <td>DEF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>għan</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>prinċipali</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ta'</td>\n",
       "      <td>GEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Conectando</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mundos</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(</td>\n",
       "      <td>X-PUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Malta</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>)</td>\n",
       "      <td>X-PUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word        POS\n",
       "0         <s>      START\n",
       "1          L-        DEF\n",
       "2        għan       NOUN\n",
       "3  prinċipali        ADJ\n",
       "4         ta'        GEN\n",
       "5  Conectando  NOUN-PROP\n",
       "6      Mundos  NOUN-PROP\n",
       "7           (      X-PUN\n",
       "8       Malta  NOUN-PROP\n",
       "9           )      X-PUN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#70 million rows\n",
    "df = pd.read_csv(os.path.join(folder,'korpus.csv'),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"},\n",
    "                 nrows=70_000_000)\n",
    "\n",
    "df_norm = pd.read_csv(os.path.join(folder,'norm_korpus.csv'),\n",
    "                      usecols=[\"Word\",\"POS\"],\n",
    "                      dtype={\"Word\": \"U\",\"POS\": \"S\"})\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc62ff14",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e312aca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['START', 'DEF', 'NOUN', 'ADJ', 'GEN', 'NOUN-PROP', 'X-PUN',\n",
       "       'X-ABV', 'PREP', 'CONJ-CORD', 'PART-PASS', 'PREP-DEF', 'PRON-PERS',\n",
       "       'COMP', 'VERB', 'END', 'LIL-DEF', 'CONJ-SUB', 'KIEN', 'GEN-PRON',\n",
       "       'ADV', 'VERB-PSEU', 'NEG', 'GEN-DEF', 'QUAN', 'PRON-DEM', 'X-DIG',\n",
       "       'PRON-INT', 'FOC', 'PREP-PRON', 'NUM-WHD', 'LIL', 'NUM-CRD',\n",
       "       'X-ENG', 'X-FOR', 'PROG', 'INT', 'X-BOR', 'PRON-PERS-NEG',\n",
       "       'LIL-PRON', 'PRON-INDEF', 'PRON-DEM-DEF', 'NUM-ORD', 'HEMM',\n",
       "       'PRON-REF', 'PART-ACT', 'FUT', 'NUM-FRC', 'PRON-REC', 'POS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28dba331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Maltese Tagset: https://mlrs.research.um.edu.mt/resources/malti03/tagset30.html\n",
    "\n",
    "df = df.drop(df[df['POS']=='X-PUN'].index) #Punctuation\n",
    "# df = df.drop(df[df['POS']=='X-DIG'].index) #Digits\n",
    "# df = df.drop(df[df['POS']=='X-ENG'].index) #English\n",
    "# df = df.drop(df[df['POS']=='X-FOR'].index) #Foreign\n",
    "# df = df.drop(df[df['POS']=='X-ABV'].index) #Abbreviations\n",
    "df = df.drop(df[df['POS']=='X-BOR'].index) #Gibberish\n",
    "df = df.drop(df[df['POS']=='INT'].index)   #Interjections\n",
    "\n",
    "\n",
    "df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-PUN'].index) #Punctuation\n",
    "# df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-DIG'].index) #Digits\n",
    "# df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-ENG'].index) #English\n",
    "# df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-FOR'].index) #Foreign\n",
    "# df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-ABV'].index) #Abbreviations\n",
    "df_norm = df_norm.drop(df_norm[df_norm['POS']=='X-BOR'].index) #Gibberish\n",
    "df_norm = df_norm.drop(df_norm[df_norm['POS']=='INT'].index)   #Interjections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3984b7",
   "metadata": {},
   "source": [
    "### Removing semi-colons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc0aebd",
   "metadata": {},
   "source": [
    "Semi colons will be used as delimters in ngrams later on. Having semicolons at the beginning or end of a word disrupts the delimiting process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6904fc92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  <s>\n",
       "1                   L-\n",
       "2                 għan\n",
       "3           prinċipali\n",
       "4                  ta'\n",
       "               ...    \n",
       "69999995          żewġ\n",
       "69999996        kwarti\n",
       "69999997          tas-\n",
       "69999998          sena\n",
       "69999999          wara\n",
       "Name: Word, Length: 62811891, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c61c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Word'] = df['Word'].apply(lambda s: ''.join(str(s).split(';')), 1)\n",
    "df_norm['Word'] = df_norm['Word'].apply(lambda s: ''.join(str(s).split(';')), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb97643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Word</th>\n",
       "      <td>Word</td>\n",
       "      <td>0.000726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>POS</th>\n",
       "      <td>POS</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     column_name  percent_missing\n",
       "Word        Word         0.000726\n",
       "POS          POS         0.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})\n",
    "\n",
    "missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff413c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Word'])\n",
    "df = df.drop(df[df[\"Word\"]=='\"'].index)\n",
    "df = df.drop(df[df[\"Word\"]=='&lt'].index)\n",
    "df = df.drop(df[df[\"Word\"]=='&gt'].index)\n",
    "df = df.drop(df[df[\"Word\"]=='&amp'].index)\n",
    "\n",
    "df_norm = df_norm.dropna(subset=['Word'])\n",
    "df_norm = df_norm.drop(df_norm[df_norm[\"Word\"]=='\"'].index)\n",
    "df_norm = df_norm.drop(df_norm[df_norm[\"Word\"]=='&lt'].index)\n",
    "df_norm = df_norm.drop(df_norm[df_norm[\"Word\"]=='&gt'].index)\n",
    "df_norm = df_norm.drop(df_norm[df_norm[\"Word\"]=='&amp'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7884382",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(folder,'korpus_clean.csv'), index=False)\n",
    "df_norm.to_csv(os.path.join(folder,'norm_korpus_clean.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628b596",
   "metadata": {},
   "source": [
    "## Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7c2c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(folder, \"korpus_clean.csv\"),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"})\n",
    "\n",
    "df_norm = pd.read_csv(os.path.join(folder, \"norm_korpus_clean.csv\"),\n",
    "                 usecols=[\"Word\",\"POS\"],\n",
    "                 dtype={\"Word\": \"U\",\"POS\": \"S\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4aa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all unique words\n",
    "df_frequency = df.value_counts().to_frame()[0].reset_index()\n",
    "df_frequency.columns = ['Word','POS','Frequency']\n",
    "\n",
    "\n",
    "df_normal_frequency = df_norm.value_counts().to_frame()[0].reset_index()\n",
    "df_normal_frequency.columns = ['Word','POS','Frequency']\n",
    "\n",
    "df_frequency.to_csv(os.path.join(folder,'korpus_frequency.csv'), index=False)\n",
    "df_normal_frequency.to_csv(os.path.join(folder,'norm_korpus_frequency.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ari2203-venv",
   "language": "python",
   "name": "ari2203-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
