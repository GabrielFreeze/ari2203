{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db847749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2599d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(),'..','data','korpus')\n",
    "vanilla = os.path.join(os.getcwd(),'..','data','korpus','ngram','vanilla')\n",
    "unk = os.path.join(os.getcwd(),'..','data','korpus','ngram','unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2096db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Lemma</th>\n",
       "      <th>Root</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;</td>\n",
       "      <td>START</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Meħtieġa</td>\n",
       "      <td>NOUN-PROP</td>\n",
       "      <td>meħtieġa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passi</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>passa</td>\n",
       "      <td>p-s-j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>konkreti</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>konkret</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dwar</td>\n",
       "      <td>PREP</td>\n",
       "      <td>dwar</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>biżżej</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>­</td>\n",
       "      <td>X-ENG</td>\n",
       "      <td>­</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>jed</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>.</td>\n",
       "      <td>X-PUN</td>\n",
       "      <td>.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>&lt;/s&gt;</td>\n",
       "      <td>END</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word        POS     Lemma   Root\n",
       "0         <s>      START       NaN    NaN\n",
       "1    Meħtieġa  NOUN-PROP  meħtieġa    NaN\n",
       "2       passi       NOUN     passa  p-s-j\n",
       "3    konkreti        ADJ   konkret    NaN\n",
       "4        dwar       PREP      dwar    NaN\n",
       "..        ...        ...       ...    ...\n",
       "360    biżżej        ADJ       NaN    NaN\n",
       "361         ­      X-ENG         ­    NaN\n",
       "362       jed       VERB       NaN    NaN\n",
       "363         .      X-PUN         .    NaN\n",
       "364      </s>        END       NaN    NaN\n",
       "\n",
       "[365 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(os.path.join(folder,'Test','Test.csv'))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7783750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.6688119602932157e-07"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def linear_interpolation(sentence:str, model:str):\n",
    "\n",
    "    if model == 'vanilla' or model == 'laplace':\n",
    "        model_path = vanilla\n",
    "    elif model == 'unk': \n",
    "        model_path = unk\n",
    "    else: raise Exception('Model does not exist!')\n",
    "        \n",
    "    #Load ngrams for the model chosen\n",
    "    print('Loading models: ', end='')\n",
    "    uni = pd.read_csv(os.path.join(model_path,'unigram.csv'))\n",
    "    bi = pd.read_csv(os.path.join(model_path,'bigram.csv'))\n",
    "    tri = pd.read_csv(os.path.join(model_path,'trigram.csv'))\n",
    "    print('[OK]')\n",
    "    \n",
    "    #Calculate size of korpus. (Number of words)\n",
    "    if model != 'vanilla':\n",
    "        path = os.path.join(folder,'norm_korpus_clean.csv')\n",
    "        korpus_size = sum(1 for _ in open(path, encoding='utf-8'))    \n",
    "    \n",
    "    words = sentence.split(' ')\n",
    "    word_count = len(words)\n",
    "    \n",
    "    if word_count < 3:\n",
    "        raise Exception('Sentence must have at least 3 words')\n",
    "    \n",
    "    #Get a numpy array of n-grams\n",
    "    uni_words = uni['Unigram'].to_numpy()\n",
    "    bi_words  = bi ['Bigram' ].to_numpy()\n",
    "    tri_words = tri['Trigram'].to_numpy()\n",
    "    \n",
    "    P = 1\n",
    "    \n",
    "    V = len(uni) #Size of the vocabulary\n",
    "    \n",
    "    λ1,λ2,λ3 = 0.1,0.3,0.6\n",
    "      \n",
    "    for i in range(2,word_count):        \n",
    "        uni_word = f'{words[i]}'\n",
    "        bi_word  = f'{words[i-1]};{words[i]}'\n",
    "        tri_word = f'{words[i-2]};{words[i-1]};{words[i]}'\n",
    "        \n",
    "        #Get the probabilities. If the model is vanilla, we don't have to get the laplace smoothed value.\n",
    "        #The function will just run into an IndexError.\n",
    "        P1 = uni[uni['Unigram'] == uni_word].iat[0,2] if model == 'vanilla' or uni_word in uni_words else 1/len(uni_words)\n",
    "        P2 = bi [bi ['Bigram']  == bi_word ].iat[0,2] if model == 'vanilla' or bi_word  in bi_words  else 1/len(bi_words)\n",
    "        P3 = tri[tri['Trigram'] == tri_word].iat[0,2] if model == 'vanilla' or tri_word in tri_words else 1/len(tri_words)\n",
    "        \n",
    "        #If the model is laplace smoothed (UNK or laplace), \n",
    "        #add 1 to the count of the ngram and \n",
    "        #add V to the denominator.\n",
    "        \n",
    "        #The laplace smoothed probability makes use of the normal vanilla probability. \n",
    "        #This is done in order to reduce code complexity and the number of nested if statements.\n",
    "        \n",
    "        if model != 'vanilla':\n",
    "            N = korpus_size\n",
    "            P1 = P1*((N+V)/N) + (1/(N+V))\n",
    "            \n",
    "            #The frequency of the previous word\n",
    "            history = f'{words[i-1]}'\n",
    "            N = uni[uni['Unigram']==history].iat[0,1] if history in uni_words else 1\n",
    "            P2 = P2*((N+V)/N) + (1/(N+V))\n",
    "            \n",
    "            #The frequency of the two previous words\n",
    "            history = f'{words[i-2]};{words[i-1]}'\n",
    "            N = bi[bi['Bigram']==history].iat[0,1] if history in bi_words else 1\n",
    "            P3 = P3*((N+V)/N) + (1/(N+V))\n",
    "        \n",
    "        #Update the current frequency of the sentenece.\n",
    "        P *= λ1*P1 + λ2*P2 + λ3*P3\n",
    "\n",
    "    return P\n",
    "    \n",
    "\n",
    "linear_interpolation('Darba f\\' għalqa sibt teżor', 'unk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714eb68e",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "990cb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(sentence:list, model:str, n:int):\n",
    "\n",
    "    if model == 'vanilla' or model == 'laplace':\n",
    "        model_path = vanilla\n",
    "    elif model == 'unk': \n",
    "        model_path = unk\n",
    "    else: raise Exception('Model does not exist!')\n",
    "    \n",
    "    if n in [1,2,3]:\n",
    "        ngram_path = ['unigram.csv','bigram.csv','trigram.csv'][n-1]\n",
    "        ngram_type = ['Unigram','Bigram','Trigram'][n-1]\n",
    "\n",
    "        print('Loading models: ', end='')\n",
    "        df_ngram = pd.read_csv(os.path.join(model_path, ngram_path))\n",
    "        ngram_words = df_ngram[ngram_type].to_numpy() #Get a numpy array of words for ngram\n",
    "\n",
    "        if n != 1:\n",
    "            prev_df_ngram = pd.read_csv(os.path.join(model_path, ['unigram.csv','bigram.csv','trigram.csv'][n-2]))\n",
    "            prev_ngram_type = ['Unigram','Bigram','Trigram'][n-2]\n",
    "            prev_ngram_words = prev_df_ngram[prev_ngram_type].to_numpy() #Get a numpy array of words for n-1gram\n",
    "        print('[OK]')\n",
    "        \n",
    "    else: raise Exception('Choose Unigram, Bigram or Trigram!')\n",
    "    \n",
    "    if model != 'vanilla':\n",
    "        path = os.path.join(folder,'korpus_clean.csv')\n",
    "        korpus_size = sum(1 for _ in open(path, encoding='utf-8'))#Size of korpus. (Number of words)\n",
    "        V = len(pd.read_csv(os.path.join(vanilla,'unigram.csv'))) #Size of the vocabulary\n",
    "    \n",
    "    P = 1 #Probability of sentences\n",
    "  \n",
    "    for i,s in enumerate(sentence):\n",
    "        words = s.split(' ')\n",
    "        word_count = len(words)\n",
    "    \n",
    "        if word_count < n:\n",
    "            raise Exception(f'Sentence must have at least {n} words')\n",
    "    \n",
    "        print(f'Sentence {i} finished..')\n",
    "        \n",
    "        for i in range(n-1,word_count):\n",
    "\n",
    "            if n == 1: word = f'{words[i]}'\n",
    "            if n == 2: word = f'{words[i-1]};{words[i]}'\n",
    "            if n == 3: word = f'{words[i-2]};{words[i-1]};{words[i]}' \n",
    "\n",
    "\n",
    "            #Get the probabilities. If the model is vanilla, we don't have to get the laplace smoothed value.\n",
    "            #The function will just run into an IndexError.\n",
    "            Pi = df_ngram[df_ngram[ngram_type] == word].iat[0,2] if model == 'vanilla' or word in ngram_words else 1/len(ngram_words)\n",
    "\n",
    "            #If the model is laplace smoothed (UNK or laplace), \n",
    "            #add 1 to the count of the ngram and \n",
    "            #add V to the denominator.\n",
    "\n",
    "            #The laplace smoothed probability makes use of the normal vanilla probability. \n",
    "            #This is done in order to reduce code complexity and the number of nested if statements.\n",
    "\n",
    "            if model != 'vanilla':\n",
    "\n",
    "                if n == 1:\n",
    "                    N = korpus_size\n",
    "\n",
    "                else:\n",
    "                    if n == 2: history = f'{words[i-1]}' #The previous word\n",
    "                    if n == 3: history = f'{words[i-2]};{words[i-1]}' #The 2 previous words\n",
    "\n",
    "                    #The frequency of the previous word/s.\n",
    "                    N = prev_df_ngram[prev_df_ngram[prev_ngram_type]==history].iat[0,1] if history in prev_ngram_words else 1\n",
    "\n",
    "                    #The probability of the current word\n",
    "                    Pi = Pi*((N+V)/N) + (1/(N+V))\n",
    "\n",
    "            #Update the current frequency of the sentenece.\n",
    "            P *= Pi\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c363941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perplexity(model:str, n:int):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "        \n",
    "    for word in df_test['Word']:\n",
    "        current_sentence.append(word)\n",
    "        \n",
    "        if word == '</s>':\n",
    "            sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []\n",
    "\n",
    "    P = get_probability(sentences[0:1],model,n)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6c89a554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "Sentence 0 finished..\n",
      "0.0407831575915836\n",
      "5.28696788530689e-07\n",
      "0.0001002601364431\n",
      "1.2576575121108814e-05\n",
      "0.0027410525318095\n",
      "0.0173651261248645\n",
      "5.142777852071248e-05\n",
      "0.0046188873979817\n",
      "0.0001266949758697\n",
      "0.000100436368706\n",
      "9.869006719239528e-05\n",
      "0.0407831415704688\n",
      "Wall time: 9.91 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5743930286565555e-41"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "calc_perplexity(model='unk',n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6ed0c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "Sentence 0 finished..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.686912360132718e+50"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability(['<s> Meħtieġa passi konkreti dwar il- qagħda fl- Isptar Mater Dei </s>'],'unk',2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ari2203-venv",
   "language": "python",
   "name": "ari2203-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
