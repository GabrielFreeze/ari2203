{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e46b7ed",
   "metadata": {},
   "source": [
    "# Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164a1743",
   "metadata": {},
   "source": [
    "The following functions calculate linear interpolation and the perplexity on a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db847749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae2599d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(os.getcwd(),'..','data','korpus')\n",
    "vanilla = os.path.join(os.getcwd(),'..','data','korpus','ngram','vanilla')\n",
    "unk = os.path.join(os.getcwd(),'..','data','korpus','ngram','unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2096db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(os.path.join(folder,'Test','Test.csv'))\n",
    "df_test = df_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7783750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "Wall time: 12.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.9477697621004495e-08"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def linear_interpolation(sentences:list, model:str):\n",
    "\n",
    "    if model == 'vanilla' or model == 'laplace':\n",
    "        model_path = vanilla\n",
    "    elif model == 'unk': \n",
    "        model_path = unk\n",
    "    else: raise Exception('Model does not exist!')\n",
    "        \n",
    "    #Load ngrams for the model chosen\n",
    "    print('Loading models: ', end='')\n",
    "    uni = pd.read_csv(os.path.join(model_path,'unigram.csv'))\n",
    "    bi = pd.read_csv(os.path.join(model_path,'bigram.csv'))\n",
    "    tri = pd.read_csv(os.path.join(model_path,'trigram.csv'))\n",
    "    print('[OK]')\n",
    "    \n",
    "    #Calculate size of korpus. (Number of words)\n",
    "    if model != 'vanilla':\n",
    "        path = os.path.join(folder,'norm_korpus_clean.csv')\n",
    "        korpus_size = sum(1 for _ in open(path, encoding='utf-8'))    \n",
    "    \n",
    "    \n",
    "    #Get a numpy array of n-grams\n",
    "    uni_words = uni['Unigram'].to_numpy()\n",
    "    bi_words  = bi ['Bigram' ].to_numpy()\n",
    "    tri_words = tri['Trigram'].to_numpy()\n",
    "\n",
    "    P = 1\n",
    "\n",
    "    V = len(uni) #Size of the vocabulary\n",
    "\n",
    "    λ1,λ2,λ3 = 0.1,0.3,0.6\n",
    "\n",
    "    for s in sentences:\n",
    "        words = s.split(' ')\n",
    "        word_count = len(words)\n",
    "\n",
    "        if word_count < 3:\n",
    "            print(f'Sentence {s} must have at least 3 words. Returning probability calculated till now.')\n",
    "            return P\n",
    "\n",
    "        \n",
    "        for i in range(2,word_count):    \n",
    "            uni_word = f'{words[i]}'\n",
    "            bi_word  = f'{words[i-1]};{words[i]}'\n",
    "            tri_word = f'{words[i-2]};{words[i-1]};{words[i]}'\n",
    "\n",
    "            #Get the probabilities. If the model is vanilla, we don't have to get the laplace smoothed value.\n",
    "            #The function will just run into an IndexError.\n",
    "            P1 = uni[uni['Unigram'] == uni_word].iat[0,2] if model == 'vanilla' or uni_word in uni_words else 1/len(uni_words)\n",
    "            P2 = bi [bi ['Bigram']  == bi_word ].iat[0,2] if model == 'vanilla' or bi_word  in bi_words  else 1/len(bi_words)\n",
    "            P3 = tri[tri['Trigram'] == tri_word].iat[0,2] if model == 'vanilla' or tri_word in tri_words else 1/len(tri_words)\n",
    "\n",
    "            #If the model is laplace smoothed (UNK or laplace), \n",
    "            #add 1 to the count of the ngram and \n",
    "            #add V to the denominator.\n",
    "\n",
    "            #The laplace smoothed probability makes use of the normal vanilla probability. \n",
    "            #This is done in order to reduce code complexity and the number of nested if statements.\n",
    "\n",
    "            if model != 'vanilla':\n",
    "                N = korpus_size\n",
    "                P1 = P1*((N+V)/N) + (1/(N+V))\n",
    "\n",
    "                #The frequency of the previous word\n",
    "                history = f'{words[i-1]}'\n",
    "                N = uni[uni['Unigram']==history].iat[0,1] if history in uni_words else 1\n",
    "                P2 = P2*((N+V)/N) + (1/(N+V))\n",
    "\n",
    "                #The frequency of the two previous words\n",
    "                history = f'{words[i-2]};{words[i-1]}'\n",
    "                N = bi[bi['Bigram']==history].iat[0,1] if history in bi_words else 1\n",
    "                P3 = P3*((N+V)/N) + (1/(N+V))\n",
    "\n",
    "            #Update the current frequency of the sentenece.\n",
    "            P *= λ1*P1 + λ2*P2 + λ3*P3\n",
    "\n",
    "    return P\n",
    "    \n",
    "\n",
    "linear_interpolation(['Darba f\\' għalqa sibt teżor'], 'unk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb9582",
   "metadata": {},
   "source": [
    "### Testing linear interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616b863",
   "metadata": {},
   "source": [
    "We can dry run through an example of linear interpolation to test it. Let's consider the sentence 'Malta hija gżira'.\n",
    "\n",
    "First we will find the ngram probabilities of the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643942a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Loading models: ', end='')\n",
    "# uni = pd.read_csv(os.path.join(model_path,'unigram.csv'))\n",
    "# bi = pd.read_csv(os.path.join(model_path,'bigram.csv'))\n",
    "# tri = pd.read_csv(os.path.join(model_path,'trigram.csv'))\n",
    "# print('[OK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714eb68e",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990cb517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(sentence:list, model:str, n:int):\n",
    "\n",
    "    if model == 'vanilla' or model == 'laplace':\n",
    "        model_path = vanilla\n",
    "    elif model == 'unk': \n",
    "        model_path = unk\n",
    "    else: raise Exception('Model does not exist!')\n",
    "    \n",
    "    if n in [1,2,3]:\n",
    "        ngram_path = ['unigram.csv','bigram.csv','trigram.csv'][n-1]\n",
    "        ngram_type = ['Unigram','Bigram','Trigram'][n-1]\n",
    "\n",
    "        print('Loading models: ', end='')\n",
    "        df_ngram = pd.read_csv(os.path.join(model_path, ngram_path))\n",
    "        ngram_words = df_ngram[ngram_type].to_numpy() #Get a numpy array of words for ngram\n",
    "\n",
    "        if n != 1:\n",
    "            prev_df_ngram = pd.read_csv(os.path.join(model_path, ['unigram.csv','bigram.csv','trigram.csv'][n-2]))\n",
    "            prev_ngram_type = ['Unigram','Bigram','Trigram'][n-2]\n",
    "            prev_ngram_words = prev_df_ngram[prev_ngram_type].to_numpy() #Get a numpy array of words for n-1gram\n",
    "        print('[OK]')\n",
    "        \n",
    "    else: raise Exception('Choose Unigram, Bigram or Trigram!')\n",
    "    \n",
    "    if model != 'vanilla':\n",
    "        path = os.path.join(folder,'korpus_clean.csv')\n",
    "        korpus_size = sum(1 for _ in open(path, encoding='utf-8'))#Size of korpus. (Number of words)\n",
    "        V = len(pd.read_csv(os.path.join(vanilla,'unigram.csv'))) #Size of the vocabulary\n",
    "    \n",
    "    P = 1 #Probability of sentences\n",
    "  \n",
    "    for sen_num,s in enumerate(sentence):\n",
    "        words = s.split(' ')\n",
    "        word_count = len(words)\n",
    "    \n",
    "        if word_count < n:\n",
    "            raise Exception(f'Sentence must have at least {n} words')\n",
    "    \n",
    "#         print(f'Sentence {sen_num} finished..')\n",
    "        \n",
    "        for i in range(n-1,word_count):\n",
    "\n",
    "            if n == 1: word = f'{words[i]}'\n",
    "            if n == 2: word = f'{words[i-1]};{words[i]}'\n",
    "            if n == 3: word = f'{words[i-2]};{words[i-1]};{words[i]}' \n",
    "\n",
    "\n",
    "            #Get the probabilities. If the model is vanilla, we don't have to get the laplace smoothed value.\n",
    "            #The function will just run into an IndexError.\n",
    "            Pi = df_ngram[df_ngram[ngram_type] == word].iat[0,2] if model == 'vanilla' or word in ngram_words else 1/len(ngram_words)\n",
    "\n",
    "            #If the model is laplace smoothed (UNK or laplace), \n",
    "            #add 1 to the count of the ngram and \n",
    "            #add V to the denominator.\n",
    "\n",
    "            #The laplace smoothed probability makes use of the normal vanilla probability. \n",
    "            #This is done in order to reduce code complexity and the number of nested if statements.\n",
    "\n",
    "            if model != 'vanilla':\n",
    "\n",
    "                if n == 1:\n",
    "                    N = korpus_size\n",
    "\n",
    "                else:\n",
    "                    if n == 2: history = f'{words[i-1]}' #The previous word\n",
    "                    if n == 3: history = f'{words[i-2]};{words[i-1]}' #The 2 previous words\n",
    "\n",
    "                    #The frequency of the previous word/s.\n",
    "                    N = prev_df_ngram[prev_df_ngram[prev_ngram_type]==history].iat[0,1] if history in prev_ngram_words else 1\n",
    "\n",
    "                    #The probability of the current word\n",
    "                    Pi = Pi*((N+V)/N) + (1/(N+V))\n",
    "\n",
    "            #Update the current probability of the sentenece.\n",
    "            P *= Pi\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30931ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_perplexity(model:str, n:int):\n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "        \n",
    "    for word in df_test['Word']:\n",
    "        current_sentence.append(word)\n",
    "        \n",
    "        if word == '</s>':\n",
    "            sentences.append(' '.join(current_sentence))\n",
    "            current_sentence = []\n",
    "    \n",
    "#     for s in sentences: print(s)\n",
    "    if n < 4:\n",
    "        P = get_probability(sentences, model,n)\n",
    "    else:\n",
    "        P = linear_interpolation(sentences, model)\n",
    "    print(P)\n",
    "    \n",
    "    P **= -1/len(sentences)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa0e2af",
   "metadata": {},
   "source": [
    "## Perplexity evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21ef2bd",
   "metadata": {},
   "source": [
    "We will calculate the perplexity of the test corpus across all variations of model an ngram. Note that n=4 represents linear inteprolation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f3a5d9",
   "metadata": {},
   "source": [
    "###### Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae6195f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "Loading models: [OK]\n",
      "Loading models: [OK]\n",
      "Loading models: [OK]\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "v1,v2,v3,v4 = 0,0,0,0\n",
    "\n",
    "try: v1 = calc_perplexity(model='vanilla',n=1)\n",
    "except Exception: pass\n",
    "try: v2 = calc_perplexity(model='vanilla',n=2)\n",
    "except Exception: pass\n",
    "try: v3 = calc_perplexity(model='vanilla',n=3)\n",
    "except Exception: pass\n",
    "try: v4 = calc_perplexity(model='vanilla',n=4)\n",
    "except Exception: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a821b15",
   "metadata": {},
   "source": [
    "###### Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec8cccb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "0.0\n",
      "Loading models: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4b9dd079ae3e>:19: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  P **= -1/len(sentences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n",
      "3.538130414718423e-226\n",
      "Loading models: [OK]\n",
      "9.293100482130701e-158\n",
      "Loading models: [OK]\n",
      "2.4721668065992436e-90\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "l1 = calc_perplexity(model='laplace',n=1)\n",
    "l2 = calc_perplexity(model='laplace',n=2)\n",
    "l3 = calc_perplexity(model='laplace',n=3)\n",
    "l4 = calc_perplexity(model='laplace',n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ca5e9",
   "metadata": {},
   "source": [
    "###### UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8c73066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models: [OK]\n",
      "0.0\n",
      "Loading models: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-4b9dd079ae3e>:19: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  P **= -1/len(sentences)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK]\n",
      "1.3539623190605953e-223\n",
      "Loading models: [OK]\n",
      "3.669375254943107e-156\n",
      "Loading models: [OK]\n",
      "1.9106911591162646e-148\n",
      "Wall time: 3min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "u1 = calc_perplexity(model='unk',n=1)\n",
    "u2 = calc_perplexity(model='unk',n=2)\n",
    "u3 = calc_perplexity(model='unk',n=3)\n",
    "u4 = calc_perplexity(model='unk',n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d0d016f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vanilla</th>\n",
       "      <th>Laplace</th>\n",
       "      <th>UNK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unigram</th>\n",
       "      <td>0</td>\n",
       "      <td>inf</td>\n",
       "      <td>inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram</th>\n",
       "      <td>0</td>\n",
       "      <td>2.305720e+56</td>\n",
       "      <td>5.213128e+55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Trigram</th>\n",
       "      <td>0</td>\n",
       "      <td>1.811173e+39</td>\n",
       "      <td>7.225235e+38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Interpolation</th>\n",
       "      <td>0</td>\n",
       "      <td>2.521916e+22</td>\n",
       "      <td>8.505550e+36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Vanilla       Laplace           UNK\n",
       "Unigram                     0           inf           inf\n",
       "Bigram                      0  2.305720e+56  5.213128e+55\n",
       "Trigram                     0  1.811173e+39  7.225235e+38\n",
       "Linear Interpolation        0  2.521916e+22  8.505550e+36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([[v1,l1,u1],\n",
    "                   [v2,l2,u2],\n",
    "                   [v3,l3,u3],\n",
    "                   [v4,l4,u4]],\n",
    "                   columns = ['Vanilla', 'Laplace', 'UNK',],\n",
    "                   index =['Unigram','Bigram','Trigram','Linear Interpolation'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe7ed4",
   "metadata": {},
   "source": [
    "From the above perplexity table, we can see that the longer the ngram the lower its perplexity. The Vanilla perplexity couldn't be calculated because the model encountered words/phrases which it did not see before. The unigram perplexity for Laplace and UNK blew up and hence were represented as infinity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ari2203-venv",
   "language": "python",
   "name": "ari2203-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
